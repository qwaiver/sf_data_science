{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://apps.skillfactory.ru/learning/course/course-v1:SkillFactory+DST-3.0+28FEB2021/block-v1:SkillFactory+DST-3.0+28FEB2021+type@sequential+block@7e517b97331f48e38ba0035df6ef8621/block-v1:SkillFactory+DST-3.0+28FEB2021+type@vertical+block@2e10c793ae1140d7bd34f39d02f13835)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Много изображений и формул, нет времени переносить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ В этом юните мы поговорим про задачи Metric Learning и основные подходы к их решению. \n",
    "\n",
    "Задачи Metric Learning основаны на измерении расстояния между объектами выборки. Такие задачи могут и не иметь явной и однозначной целевой метки, поэтому Metric Learning может использоваться как при обучении с учителем (supervised learning), так и без него (unsupervised learning).\n",
    "\n",
    "К основным постановкам задач Metric Learning относят задачи кластеризации, задачи поиска ближайших соседей, задачи снижения размерности и частичного восстановления данных. Далее мы постараемся разобрать на примере практическое применение методов для задачи снижения размерности и кластеризации. \n",
    "\n",
    "Для начала поговорим поговорим про расстояния, которые рассматриваются в задачах Metric Learning. Прежде всего стоит объяснить, что такое расстояние в контексте машинного обучения. \n",
    "\n",
    "Каждый объект выборки можно представить как точку в -мерном пространстве, где  — количество признаков, которые описывают объект. Тогда расстояние есть некоторая мера дистанции между объектами в -мерном пространстве (расстояние между n-мерными векторами, описывающими объекты). \n",
    "\n",
    "На иллюстрации приведена визуализация объектов, описываемых двухмерным пространством (количество признаков равно двум). Мы можем визуализировать также трехмерное пространство и одномерное, однако размерности выше n >3 визуализировать уже сложно.\n",
    "\n",
    "\n",
    "\n",
    "Для понимания расстояния между двумя точками прибегнем к помощи ещё одной иллюстрации: \n",
    "\n",
    "\n",
    "\n",
    "На рисунке представлено расстояние между двумя точками A и Б уже в трёхмерном пространстве и обозначено как dist(A,B).\n",
    "\n",
    "В данных задачах используется расстояние Махаланобиса, обозначаемое далее в формуле как , между объектами  и   — объектами выборки, описываемыми признаковым пространством (на практике это будут две строки в датасете длиной, равной количеству признаков), вводимое как:\n",
    "\n",
    "\n",
    "\n",
    "где  — матрица преобразования пространства, например понижение размерности. \n",
    "\n",
    "Пример. Самое простое преобразование пространства, известное ещё со школы — это поворот вокруг одной из осей в трехмерном пространстве на угол . \n",
    "\n",
    "В матричном виде преобразование для каждого вектора данного пространства  в повернутый на угол новый вектор  имеет вид: \n",
    "\n",
    ", где в скалярном виде вектор имеет вид , а матрица поворота вокруг оси  имеет вид:\n",
    "\n",
    "\n",
    "\n",
    "Размерность матрицы  равна , где  — размерность преобразованного пространства,  — количество признаков. В случае уменьшения размерности справедливо следующее неравенство:\n",
    "\n",
    "В частном случае, когда  представляет  единичную матрицу, то есть изменений пространства нет и используется для расчета исходное, то такое расстояние является Евклидовым расстоянием. Из свойства единичной матрицы имеем для векторного вида евклидового расстояния:\n",
    "\n",
    "\n",
    "\n",
    "В скалярном виде:\n",
    "\n",
    "\n",
    "\n",
    "Примечание. Единичной матрицей размера  называют квадратную матрицу , у которой ненулевые  элементы лежат на главной диагонали и равны единице. Главная диагональ — диагональ матрицы, проведённая из верхнего левого в нижний правый угол.\n",
    "\n",
    "Также в задачах Metric Learning используются альтернативные метрики. Рассмотрим некоторые из них.\n",
    "\n",
    "Расстояние Миньковского обозначим как :\n",
    "\n",
    ", где  — некоторый параметр ().\n",
    "\n",
    "При  расстояние Миньковского превращается в расстояние Чебышева:\n",
    "\n",
    "\n",
    "\n",
    "При  — расстояние Манхэттена:\n",
    "\n",
    "или L1-расстояние.\n",
    "\n",
    "При  — Евклидово расстояние, L2-расстояние:\n",
    "\n",
    "\n",
    "\n",
    "Косинусное расстояние (далее обозначено как ) — расстояние между векторами  и , описывающими объекты выборки  и .\n",
    "\n",
    "\n",
    "\n",
    "где\n",
    "\n",
    "\n",
    "\n",
    "— Евклидова норма -мерного вектора. \n",
    "\n",
    "МЕТОД БЛИЖАЙШИХ СОСЕДЕЙ. КЛАСТЕРИЗАЦИЯ\n",
    "\n",
    "Прежде чем перейти к рассмотрению метода решения данных типов задач, необходимо понять спектр практических задач, которые могут быть решены этими подходами. \n",
    "\n",
    "В модуле ML-4 мы подробно изучили основные примеры кластеризации. Напомним, что с помощью кластеризации можно разделить пользователей магазина на группы схожих между собой покупателей, можно собрать пользователей сайта в группы по интересам. \n",
    "\n",
    "Использование кластеризации здесь обусловлено тем, что у нас нет правильных меток, по которым мы можем классифицировать людей. Кроме того, с помощью кластеризации можно определить тему текста или отнести его к определенной тематике, выяснить, является ли отзыв на сайте позитивным или негативным. Кластеризация также часто используется в медицине, особенно в генетике. \n",
    "\n",
    "Основная сложность задачи кластеризации заключается в том, что не всегда известно истинное значение количества кластеров — иногда надо это исследовать.\n",
    "\n",
    "Основным методом решения задачи кластеризации является метод k-means, с которым мы познакомились ранее. Далее мы попробуем решить с вами реальную задачу на определение кластеров покупателей, где нам будет необходимо определить количество кластеров.\n",
    "\n",
    "Задача поиска ближайших соседей (k Nearest Neighbours, kNN) очень схожа с задачей кластеризации. Основное отличие заключается в том, что кластеризация — задача разбиения на кластеры, с последующим описанием и интерпретацией общих черт кластера. В свою очередь, поиск ближайших соседей состоит в определении уже существующего кластера, к которому относится объект. \n",
    "\n",
    "Другими словами, поиск ближайших соседей используется для задачи обучения с учителем, где целевые метки предварительно могут быть получены, например, кластеризацией. \n",
    "\n",
    "Задачи обучения с учителем, где целевые метки получены в результате кластеризации, называются слабым обучением с учителем (weak supervised learning). \n",
    "\n",
    "Давайте подробнее разберём работу алгоритма. \n",
    "\n",
    "АЛГОРИТМ РАБОТЫ K-NEAREST NEIGHBOUR (KNN)\n",
    "\n",
    "Зададим  — количество ближайший соседей, по которым принимается решение.\n",
    "Выберем расстояние, по которому будут определяться соседи. Обычно это евклидово расстояние, рассмотренное ранее.\n",
    "Обучим модель на признаковом пространстве.\n",
    "Как было сказано ранее, обучение алгоритма kNN заключается в получении пар признаковое пространство-целевая метка. Далее данные нормализуются с использованием MinMaxScaler(). Напомним его формулу:\n",
    "\n",
    "Получим предсказание модели.\n",
    "Предсказание заключается в поиске k ближайших соседей на основе метрики, выбранной ранее. Данная модель в sklearn реализована в модуле sklearn.neighbors как KNeighborsClassifier для задачи классификации и KNeighborsRegressor для задачи регресии. \n",
    "\n",
    "Для понимания  работы получения предсказания обратимся к иллюстрации: \n",
    "\n",
    "\n",
    "\n",
    "На рисунке представлена задача бинарной классификации с двумя классами (синие квадраты и красные треугольники). Для нашего объекта, отмеченного зелёным кругом, нам нужно определить класс. \n",
    "\n",
    "Стоит отметить, что если k = 3 (в sklearn данный параметр главный и называется n_neighbors), то в радиус трёх ближайших соседей попадают два красных треугольника и один синий квадрат (на рисунке это чёрная линия). Радиус в модели определяется евклидовым расстоянием от классифицируемого объекта, которое рассчитывается различными методами (в sklearn параметр algorigthm — см. параметры алгоритма), в том числе и жадным алгоритмом. Итак, большинство представляют объекты класса красных треугольников, и стоит принять класс нашего объекта за класс красных треугольников. В данном случае вероятность верного определения класса равна 0.66 (2/3). \n",
    "\n",
    "Однако, если изначально k = 5, то в этом радиусе пятью ближайшими соседями (отмечено чёрным пунктиром) являются уже три синих квадрата и два красных треугольника. Тогда класс объекта определяется как наиболее часто встречаемый класс среди соседей, и объект, вероятнее всего, принадлежит квадратам — в данном случае с вероятностью 0.6, если при предсказании не учитывать расстояние. В случае учёта расстояния (параметр weights, значение distance) при k=5 может случиться так, что наш объект будет классифицирован как красный треугольник, так как у этих двух объектов будет больший вес по причине сильной близости к объекту.\n",
    "\n",
    "Итак, в ходе данного примера мы узнали смысл некоторых параметров KNeighborsClassifier. Давайте подробнее разберём все параметры модели:\n",
    "\n",
    "n_neighbors — количество  соседей, по умолчанию 5.\n",
    "weights{'uniform', 'distance'} — вес объекта в зависимости от удаленности от объекта. Как говорилось ранее, если uniform, то расстояние не будет учитываться в подсчёте вероятности для классификации. По умолчанию оно не учитывается, параметр имеет значение 'uniform'. При 'distance' расстояние учитывается.\n",
    "algorithm — алгоритм, используемый при подсчёте расстояния.\n",
    "'ball_tree' использует BallTree.\n",
    "'kd_tree' использует  KDTree.\n",
    "'brute' использует жадный алгоритм.\n",
    "'auto' выбирает оптимальный из вышепредставленных.\n",
    "leaf_size — параметр для поиска расстояния BallTree или KDTree. По умолчанию равен 30.\n",
    "p — значение p-метрики Миньковского. Как мы знаем,  — расстояние Манхэттена (L1),  — Евклидово расстояние (L2). По умолчанию .\n",
    "metric — метрика для оценки расстояний, по умолчанию расстояние Миньковского.\n",
    "n_jobs — количество задач, выполняемых параллельно, ограничено количеством ядер вычислительной машины. При n_jobs=-1 использует все.\n",
    "Приведём пример: \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    " \n",
    "\n",
    "Сформируем искусственные данные: \n",
    "\n",
    "data = pd.DataFrame([[0,0,0],[0.2,0.1,0], [1,0,0], [2,1,1],[2.5,0.9,1], [3,1,1]], columns = ['x','y', 'target'])\n",
    " \n",
    "\n",
    "Отделим матрицу признаков и вектор-столбец правильных ответов: \n",
    "\n",
    "X = data.drop(['target'],axis = 1)\n",
    "Y = data['target']\n",
    "Обучим kNN с тремя соседями:\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, Y)\n",
    " \n",
    "\n",
    "Посмотрим на визуализацию исходных данных и работы модели по классификации точки. Вероятность принадлежности к классу можно вычислить методом predict_proba(), а само предсказание — методом predict:\n",
    "\n",
    "for item in np.linspace(0,3,10):\n",
    "    x = item\n",
    "    y = item*0.25\n",
    "    plt.figure(figsize = (5,5))\n",
    "    red = data[data.target == 1]\n",
    "    blue = data[data.target == 0]\n",
    "    red_prob = round(neigh.predict_proba([[x,y]])[0][1],2)*100 #вероятность принадлежности к красным\n",
    "    blue_prob = round(neigh.predict_proba([[x,y]])[0][0],2)*100 #вероятность принадлежности к синим\n",
    "    plt.title(f'Красный на {red_prob} %, синий на {blue_prob} %')\n",
    "    plt.xlabel('значения признака x')\n",
    "    plt.ylabel('значение признака y')\n",
    "    plt.scatter(red.x.values,red.y.values ,c= 'r')\n",
    "    plt.scatter(blue.x.values,blue.y.values ,c= 'b')\n",
    "    plt.scatter(x,y ,c= 'g')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    " \n",
    "\n",
    "Анимированный результат кода:\n",
    "\n",
    "\n",
    "\n",
    "Как можно видеть, когда зеленая точка «улетает» из кластера синих точек, вероятность падает, при этом вероятность принадлежности к классу в данном случае может принимать всего четыре значения (0, 0.33, 0.66, 1) в зависимости от количества соседей, так как не учтено расстояние при подсчёте вероятности. \n",
    "\n",
    "Попробуем учесть расстояние. Для этого в модель подадим параметр weights='distance'.\n",
    "\n",
    "В коде выше изменим объект модели:\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "neigh.fit(X, Y)\n",
    "И выполним код заново. Анимированный результат работы модели: \n",
    "\n",
    "\n",
    "\n",
    "Как видим, теперь вероятности могут принимать абсолютно любые значения и точнее предсказывать вероятность принадлежности к классу.\n",
    "\n",
    "Примечание. В случае задачи регрессии целевая метка нашего объекта определяется как среднее значение целевой метки по k ближайшим соседям.\n",
    "\n",
    "Расчёты ведутся точно так же, только вместо вероятности предсказания класса высчитывается среднее по ближайшим соседям. Также при использовании взвешенного подхода, среднее уже будет взвешенным в зависимости от расстояния до соседа: чем ближе к точке, тем больший вклад соседа.\n",
    "\n",
    "Предположим, целевая метка у трёх ближайших соседей в нашем случае равна 0.2, 0.3 и 0.8 соответственно, целевая метка без учёта расстояния равна (0.2+0.3+0.8)/3 = 0.43. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
